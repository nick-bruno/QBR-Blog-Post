{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af0900fb",
   "metadata": {},
   "source": [
    "# Scrape Statheads Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116ce0e5",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9692d0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "/additional_web_scraping_scripts\n",
    "/data\n",
    "/stathead_credentials\n",
    "/.ipynb_checkpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8eae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f984552",
   "metadata": {},
   "source": [
    "### Read in Statheads Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6071a055",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./stathead_credentials/stathead_credentials.txt', \"r\") as text_file:\n",
    "    creds = text_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8623fd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_username = creds[0].split('=')[1].strip('')\n",
    "stats_password = creds[1].split('=')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429c9f2a",
   "metadata": {},
   "source": [
    "### Scrape from Statheads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eea436a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def login_to_stathead(driver, username, password):\n",
    "    \"\"\"Handle the login process for Stathead\"\"\"\n",
    "    print(\"Attempting to log in to Stathead...\")\n",
    "    \n",
    "    wait = WebDriverWait(driver, 15)\n",
    "    \n",
    "    try:\n",
    "        # Go to the login page first\n",
    "        login_url = \"https://stathead.com/users/login.cgi\"\n",
    "        driver.get(login_url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        print(\"Looking for login form...\")\n",
    "        \n",
    "        # Find username field\n",
    "        username_selectors = [\n",
    "            \"input[name='username']\",\n",
    "            \"input[name='email']\", \n",
    "            \"input[type='email']\",\n",
    "            \"input[id='username']\",\n",
    "            \"input[id='email']\"\n",
    "        ]\n",
    "        \n",
    "        username_field = None\n",
    "        for selector in username_selectors:\n",
    "            try:\n",
    "                username_field = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, selector)))\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if not username_field:\n",
    "            raise Exception(\"Username field not found\")\n",
    "        \n",
    "        # Find password field\n",
    "        password_field = driver.find_element(By.CSS_SELECTOR, \"input[type='password']\")\n",
    "        \n",
    "        # Enter credentials\n",
    "        username_field.clear()\n",
    "        username_field.send_keys(username)\n",
    "        password_field.clear()\n",
    "        password_field.send_keys(password)\n",
    "        \n",
    "        # Submit form\n",
    "        password_field.send_keys(Keys.RETURN)\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # Check if login successful\n",
    "        if 'logout' in driver.page_source.lower() or 'account' in driver.page_source.lower():\n",
    "            print(\"✅ Login successful!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"❌ Login may have failed\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Login error: {e}\")\n",
    "        return False\n",
    "\n",
    "def find_next_page_button(driver):\n",
    "    \"\"\"Find the Next Page button using multiple strategies\"\"\"\n",
    "    \n",
    "    next_page_selectors = [\n",
    "        # Text-based searches\n",
    "        \"//a[contains(text(), 'Next')]\",\n",
    "        \"//button[contains(text(), 'Next')]\",\n",
    "        \"//a[contains(text(), 'next')]\",\n",
    "        \"//button[contains(text(), 'next')]\",\n",
    "        \"//a[contains(text(), 'Next Page')]\",\n",
    "        \"//button[contains(text(), 'Next Page')]\",\n",
    "        \n",
    "        # Common pagination patterns\n",
    "        \"//a[@title='Next Page']\",\n",
    "        \"//button[@title='Next Page']\",\n",
    "        \"//a[@title='Next']\",\n",
    "        \"//button[@title='Next']\",\n",
    "        \n",
    "        # Class-based searches (common pagination classes)\n",
    "        \"//a[contains(@class, 'next')]\",\n",
    "        \"//button[contains(@class, 'next')]\",\n",
    "        \"//a[contains(@class, 'page-next')]\",\n",
    "        \"//button[contains(@class, 'page-next')]\",\n",
    "        \n",
    "        # Arrow symbols\n",
    "        \"//a[contains(text(), '→')]\",\n",
    "        \"//button[contains(text(), '→')]\",\n",
    "        \"//a[contains(text(), '>')]\",\n",
    "        \"//button[contains(text(), '>')]\",\n",
    "        \n",
    "        # Rel attribute (HTML standard for pagination)\n",
    "        \"//a[@rel='next']\",\n",
    "        \n",
    "        # Sports Reference specific patterns\n",
    "        \"//a[contains(@href, 'offset=')]\",\n",
    "        \"//a[contains(@href, 'page=')]\"\n",
    "    ]\n",
    "    \n",
    "    for selector in next_page_selectors:\n",
    "        try:\n",
    "            elements = driver.find_elements(By.XPATH, selector)\n",
    "            for element in elements:\n",
    "                # Check if element is visible and clickable\n",
    "                if element.is_displayed() and element.is_enabled():\n",
    "                    # Additional check - make sure it's not disabled\n",
    "                    classes = element.get_attribute('class') or ''\n",
    "                    if 'disabled' not in classes.lower():\n",
    "                        return element\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return None\n",
    "\n",
    "def scrape_page_table(driver, page_num):\n",
    "    \"\"\"Scrape table data from current page\"\"\"\n",
    "    print(f\"Scraping page {page_num}...\")\n",
    "    \n",
    "    # Wait for table to load\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    time.sleep(2)  # Give page time to fully load\n",
    "    \n",
    "    # Find the main data table\n",
    "    table_selectors = [\n",
    "        \"#stats\",\n",
    "        \"#results\", \n",
    "        \".stats_table\",\n",
    "        \".sortable\",\n",
    "        \"table[id*='stats']\",\n",
    "        \"table[class*='stats']\"\n",
    "    ]\n",
    "    \n",
    "    table = None\n",
    "    for selector in table_selectors:\n",
    "        try:\n",
    "            tables = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "            if tables:\n",
    "                table = max(tables, key=lambda t: len(t.find_elements(By.TAG_NAME, \"tr\")))\n",
    "                break\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if not table:\n",
    "        print(f\"❌ No table found on page {page_num}\")\n",
    "        return []\n",
    "    \n",
    "    # Extract data rows (skip headers since we'll get them from first page)\n",
    "    data_rows = []\n",
    "    try:\n",
    "        tbody = table.find_element(By.TAG_NAME, \"tbody\")\n",
    "        rows = tbody.find_elements(By.TAG_NAME, \"tr\")\n",
    "    except:\n",
    "        # Fallback: get all rows\n",
    "        all_rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "        # Skip first row if it contains only th elements (header)\n",
    "        rows = []\n",
    "        for row in all_rows:\n",
    "            if row.find_elements(By.TAG_NAME, \"td\"):  # Has data cells\n",
    "                rows.append(row)\n",
    "    \n",
    "    for i, row in enumerate(rows):\n",
    "        try:\n",
    "            # Skip rows that are just headers\n",
    "            if row.find_elements(By.TAG_NAME, \"th\") and not row.find_elements(By.TAG_NAME, \"td\"):\n",
    "                continue\n",
    "            \n",
    "            cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "            if not cells:\n",
    "                continue\n",
    "            \n",
    "            row_data = []\n",
    "            for cell in cells:\n",
    "                cell_text = cell.text.strip()\n",
    "                \n",
    "                # Handle links\n",
    "                if not cell_text:\n",
    "                    link = cell.find_elements(By.TAG_NAME, \"a\")\n",
    "                    if link:\n",
    "                        cell_text = link[0].text.strip()\n",
    "                \n",
    "                # Handle data attributes\n",
    "                if not cell_text:\n",
    "                    cell_text = cell.get_attribute('data-stat') or ''\n",
    "                \n",
    "                row_data.append(cell_text)\n",
    "            \n",
    "            if row_data:\n",
    "                data_rows.append(row_data)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {i} on page {page_num}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"✅ Page {page_num}: extracted {len(data_rows)} rows\")\n",
    "    return data_rows\n",
    "\n",
    "def get_table_headers(driver):\n",
    "    \"\"\"Extract table headers from the first page\"\"\"\n",
    "    table_selectors = [\n",
    "        \"#stats\",\n",
    "        \"#results\", \n",
    "        \".stats_table\",\n",
    "        \".sortable\",\n",
    "        \"table[id*='stats']\",\n",
    "        \"table[class*='stats']\"\n",
    "    ]\n",
    "    \n",
    "    table = None\n",
    "    for selector in table_selectors:\n",
    "        try:\n",
    "            tables = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "            if tables:\n",
    "                table = max(tables, key=lambda t: len(t.find_elements(By.TAG_NAME, \"tr\")))\n",
    "                break\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if not table:\n",
    "        return []\n",
    "    \n",
    "    headers = []\n",
    "    try:\n",
    "        thead = table.find_element(By.TAG_NAME, \"thead\")\n",
    "        header_rows = thead.find_elements(By.TAG_NAME, \"tr\")\n",
    "        header_row = header_rows[-1]  # Last header row usually has column names\n",
    "        header_cells = header_row.find_elements(By.TAG_NAME, \"th\")\n",
    "        \n",
    "        for cell in header_cells:\n",
    "            header_text = cell.text.strip()\n",
    "            if not header_text:\n",
    "                header_text = cell.get_attribute('data-stat') or cell.get_attribute('aria-label') or ''\n",
    "            headers.append(header_text)\n",
    "    except:\n",
    "        # Fallback: use first data row structure to determine column count\n",
    "        try:\n",
    "            first_data_row = table.find_element(By.XPATH, \".//tr[td]\")\n",
    "            cells = first_data_row.find_elements(By.TAG_NAME, \"td\")\n",
    "            headers = [f\"Column_{i+1}\" for i in range(len(cells))]\n",
    "        except:\n",
    "            headers = []\n",
    "    \n",
    "    return headers\n",
    "\n",
    "def scrape_all_pages(username, password, url):\n",
    "    \"\"\"Scrape all pages by clicking Next until no more pages\"\"\"\n",
    "    \n",
    "    # Set up Chrome options\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "    chrome_options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
    "    \n",
    "    # Keep browser visible for debugging\n",
    "    # chrome_options.add_argument(\"--headless\")\n",
    "    \n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "    \n",
    "    all_data = []\n",
    "    headers = []\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Login\n",
    "        login_success = login_to_stathead(driver, username, password)\n",
    "        if not login_success:\n",
    "            print(\"Login failed, cannot continue\")\n",
    "            return None\n",
    "        \n",
    "        # Step 2: Navigate to first page\n",
    "        print(f\"Navigating to data page...\")\n",
    "        driver.get(url)\n",
    "        time.sleep(5)\n",
    "        \n",
    "    \n",
    "        # Step 3: Get headers from first page\n",
    "        print(\"Extracting table headers...\")\n",
    "        headers = get_table_headers(driver)\n",
    "        if not headers:\n",
    "            print(\"❌ Could not extract table headers\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Found {len(headers)} columns: {headers[:5]}...\" if len(headers) > 5 else f\"Found {len(headers)} columns: {headers}\")\n",
    "        \n",
    "        # Step 4: Scrape all pages\n",
    "        page_num = 1\n",
    "        max_pages = 1000  # Safety limit to prevent infinite loops\n",
    "        \n",
    "        while page_num <= max_pages:\n",
    "            print(f\"\\n--- PAGE {page_num} ---\")\n",
    "            \n",
    "            # Scrape current page\n",
    "            page_data = scrape_page_table(driver, page_num)\n",
    "            if page_data:\n",
    "                all_data.extend(page_data)\n",
    "                print(f\"Total rows collected so far: {len(all_data)}\")\n",
    "            else:\n",
    "                print(f\"No data found on page {page_num}\")\n",
    "            \n",
    "            # Look for Next Page button\n",
    "            print(\"Looking for Next Page button...\")\n",
    "            next_button = find_next_page_button(driver)\n",
    "            \n",
    "            if next_button:\n",
    "                try:\n",
    "                    button_text = next_button.text.strip() or next_button.get_attribute('title') or 'Next'\n",
    "                    print(f\"✅ Found Next Page button: '{button_text}'\")\n",
    "                    \n",
    "                    # Scroll to button and click\n",
    "                    driver.execute_script(\"arguments[0].scrollIntoView(true);\", next_button)\n",
    "                    time.sleep(1)\n",
    "                    \n",
    "                    # Click the button\n",
    "                    next_button.click()\n",
    "                    print(\"✅ Clicked Next Page button\")\n",
    "                    \n",
    "                    # Wait for next page to load\n",
    "                    time.sleep(3)\n",
    "                    \n",
    "                    page_num += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Error clicking Next Page button: {e}\")\n",
    "                    break\n",
    "            else:\n",
    "                print(\"🏁 No Next Page button found - reached end of data\")\n",
    "                break\n",
    "        \n",
    "        if page_num > max_pages:\n",
    "            print(f\"⚠️ Reached maximum page limit ({max_pages})\")\n",
    "        \n",
    "        print(f\"\\n✅ Scraping complete! Total pages: {page_num-1}\")\n",
    "        print(f\"Total rows collected: {len(all_data)}\")\n",
    "        \n",
    "        # Step 5: Create DataFrame\n",
    "        if not all_data:\n",
    "            print(\"❌ No data collected\")\n",
    "            return None\n",
    "        \n",
    "        # Ensure all rows have same number of columns\n",
    "        max_cols = len(headers) if headers else max(len(row) for row in all_data)\n",
    "        \n",
    "        # Pad headers if necessary\n",
    "        while len(headers) < max_cols:\n",
    "            headers.append(f\"Column_{len(headers) + 1}\")\n",
    "        \n",
    "        # Pad data rows if necessary\n",
    "        for row in all_data:\n",
    "            while len(row) < max_cols:\n",
    "                row.append(\"\")\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(all_data, columns=headers[:max_cols])\n",
    "        \n",
    "        # Clean data\n",
    "        print(\"Cleaning data...\")\n",
    "        df = df.dropna(axis=1, how='all')  # Remove empty columns\n",
    "        df = df.dropna(axis=0, how='all')  # Remove empty rows\n",
    "        \n",
    "        # Strip whitespace\n",
    "        for col in df.select_dtypes(include=['object']):\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "        \n",
    "        print(f\"✅ Final dataset: {len(df)} rows × {len(df.columns)} columns\")\n",
    "        \n",
    "        # Display sample\n",
    "        print(\"\\nFirst 5 rows:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        print(f\"\\nLast 5 rows:\")\n",
    "        print(df.tail())\n",
    "        \n",
    "        # Save to CSV\n",
    "        output_file = \"nfl_qb_stats_all_pages.csv\"\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"\\n💾 All data saved to: {output_file}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during multi-page scraping: {e}\")\n",
    "        \n",
    "        # Save whatever data we have\n",
    "        if all_data:\n",
    "            try:\n",
    "                df = pd.DataFrame(all_data)\n",
    "                df.to_csv(\"nfl_qb_stats_partial.csv\", index=False)\n",
    "                print(f\"💾 Partial data saved (rows: {len(all_data)})\")\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Screenshot for debugging\n",
    "        try:\n",
    "            driver.save_screenshot(\"debug_screenshot.png\")\n",
    "            print(\"Screenshot saved: debug_screenshot.png\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    finally:\n",
    "        input(\"\\nPress Enter to close browser...\")\n",
    "        driver.quit()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    print(\"=== Stathead Multi-Page Table Scraper ===\")\n",
    "    #print(\"This will scrape ALL pages by automatically clicking 'Next Page'\")\n",
    "    #print()\n",
    "    \n",
    "    # URL to scrape  \n",
    "    url = \"https://stathead.com/football/player-game-finder.cgi?request=1&match=player_game&order_by=pass_rating&year_min=2006&year_max=2024&week_num_season_min=1&week_num_season_max=18&ccomp%5B2%5D=gt&cval%5B2%5D=1&cstat%5B2%5D=pass_att\"\n",
    "    \n",
    "    # Get credentials\n",
    "    username = stats_username\n",
    "    password = stats_password\n",
    "    \n",
    "    if not username or not password:\n",
    "        print(\"Username and password are required!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nStarting multi-page scrape for user: {username}\")\n",
    "    print(\"This may take several minutes depending on the number of pages...\")\n",
    "    \n",
    "    df = scrape_all_pages(username, password, url)\n",
    "    \n",
    "    if df is not None:\n",
    "        print(f\"\\n🎉 SUCCESS! Scraped {len(df)} total rows across all pages\")\n",
    "        print(f\"Data shape: {df.shape}\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Show some statistics\n",
    "        if 'Player' in df.columns:\n",
    "            unique_players = df['Player'].nunique()\n",
    "            print(f\"Unique players: {unique_players}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"\\n❌ Multi-page scraping failed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
